ok now, i will give you every terraform file for each folder, i want you to analyse them all and tell me how i can re arange it for things to be in the right place, and how we can rea adjust it for reutilization, modularization, and optimization:

Networking:

main.tf

data "aws_caller_identity" "current" {}

# VPC (Virtual Private Cloud)
resource "aws_vpc" "main" {
  cidr_block = var.vpc_cidr

  tags = {
    Name = "main-vpc"
  }
}

# Public Subnet 1
resource "aws_subnet" "public1" {
  vpc_id                  = aws_vpc.main.id
  cidr_block              = var.subnet_cidr1
  availability_zone       = var.availability_zone1
  map_public_ip_on_launch = true

  tags = {
    Name = "public-subnet-1"
  }
}

# Public Subnet 2
resource "aws_subnet" "public2" {
  vpc_id                  = aws_vpc.main.id
  cidr_block              = var.subnet_cidr2
  availability_zone       = var.availability_zone2
  map_public_ip_on_launch = true

  tags = {
    Name = "public-subnet-2"
  }
}

# Admin Subnet
resource "aws_subnet" "admin" {
  vpc_id                  = aws_vpc.main.id
  cidr_block              = var.admin_subnet_cidr
  map_public_ip_on_launch = true

  tags = {
    Name = "admin-subnet"
  }
}

# Internet Gateway
resource "aws_internet_gateway" "igw" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = "main-igw"
  }
}

# Route Table
resource "aws_route_table" "routetable" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.igw.id
  }

  tags = {
    Name = "main-route-table"
  }
}

# Route Table Association for Public Subnet 1
resource "aws_route_table_association" "public_association1" {
  subnet_id      = aws_subnet.public1.id
  route_table_id = aws_route_table.routetable.id
}

# Route Table Association for Public Subnet 2
resource "aws_route_table_association" "public_association2" {
  subnet_id      = aws_subnet.public2.id
  route_table_id = aws_route_table.routetable.id
}

# Route Table Association for Admin Subnet
resource "aws_route_table_association" "admin_association" {
  subnet_id      = aws_subnet.admin.id
  route_table_id = aws_route_table.routetable.id
}

resource "aws_security_group" "elb" {
  vpc_id = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "elb-security-group"
  }
}

resource "aws_security_group" "instance" {
  vpc_id = aws_vpc.main.id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 6443
    to_port     = 6443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 3100
    to_port     = 3100
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 2379
    to_port     = 2380
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 10250
    to_port     = 10250
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Calico ports
  ingress {
    from_port   = 179
    to_port     = 179
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 4789
    to_port     = 4789
    protocol    = "udp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 4
    to_port     = 4
    protocol    = "4"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 8285
    to_port     = 8285
    protocol    = "udp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 9100
    to_port     = 9100
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 8472
    to_port     = 8472
    protocol    = "udp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 30000
    to_port     = 32767
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 500
    to_port     = 500
    protocol    = "udp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 1194
    to_port     = 1194
    protocol    = "udp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 4500
    to_port     = 4500
    protocol    = "udp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "instance-security-group"
  }
}

resource "aws_lb_target_group" "k8s_target_group" {
  name        = "k8s-target-group"
  port        = 80
  protocol    = "HTTP"
  vpc_id      = aws_vpc.main.id
  target_type = "instance"

  health_check {
    interval            = 30
    path                = "/"
    port                = "traffic-port"
    protocol            = "HTTP"
    timeout             = 5
    unhealthy_threshold = 2
    healthy_threshold   = 2
  }

  tags = {
    Name = "k8s-target-group"
  }
}

variables.tf

variable "vpc_cidr" {
  description = "CIDR block for the VPC"
  type        = string
  default     = "10.0.0.0/16"
}

variable "subnet_cidr1" {
  description = "CIDR block for the first public subnet"
  type        = string
  default     = "10.0.1.0/24"
}

variable "subnet_cidr2" {
  description = "CIDR block for the second public subnet"
  type        = string
  default     = "10.0.2.0/24"
}

variable "admin_subnet_cidr" {
  description = "CIDR block for the admin subnet"
  type        = string
  default     = "10.0.3.0/24"
}

variable "availability_zone1" {
  description = "Availability zone for the first public subnet"
  type        = string
  default     = "eu-north-1a"
}

variable "availability_zone2" {
  description = "Availability zone for the second public subnet"
  type        = string
  default     = "eu-north-1b"
}

variable "region" {
  description = "AWS region"
  type        = string
  default     = "eu-north-1"
}


outputs.tf

output "public_subnet_id1" {
  value = aws_subnet.public1.id
}

output "public_subnet_id2" {
  value = aws_subnet.public2.id
}

output "admin_subnet_id" {
  value = aws_subnet.admin.id
}

output "instance_security_group_id" {
  value = aws_security_group.instance.id
}

output "lb_target_group_arn" {
  value = aws_lb_target_group.k8s_target_group.arn
}


-----------------

Admin

main.tf

provider "aws" {
  region = var.region
}

data "terraform_remote_state" "networking" {
  backend = "local"

  config = {
    path = "../networking/terraform.tfstate"
  }
}

resource "aws_instance" "admin" {
  ami           = "ami-052387465d846f3fc"  # Change to an appropriate AMI ID for your region
  instance_type = var.instance_type
  subnet_id     = data.terraform_remote_state.networking.outputs.admin_subnet_id
  vpc_security_group_ids = [data.terraform_remote_state.networking.outputs.instance_security_group_id]
  key_name      = var.key_name
  associate_public_ip_address = true

  user_data = file("/home/ec2-user/devops_setup/prepare_env.sh")

  tags = {
    Name  = "Admin-Server"
    Group = "Admin"
  }
}

resource "aws_eip" "admin_eip" {
  instance = aws_instance.admin.id
}

output "admin_public_ip" {
  value = aws_eip.admin_eip.public_ip
}


variables.tf

variable "region" {
  description = "AWS region"
  type        = string
  default     = "eu-north-1"
}

variable "instance_type" {
  description = "Type of EC2 instance"
  type        = string
  default     = "t3.medium"
}

variable "key_name" {
  description = "Name of the SSH key pair"
  type        = string
  default     = "devOps_training"  # Replace with your actual key pair name
}

variable "user_public_keys" {
  description = "Map of user names to their public keys"
  type        = map(string)
  default     = {}
}


outputs.tf

output "admin_server_private_ip" {
  value = aws_instance.admin.private_ip
}


--------------------------------

k8s_cluster

main.tf

data "terraform_remote_state" "networking" {
  backend = "local"
  config = {
    path = "../networking/terraform.tfstate"
  }
}

data "terraform_remote_state" "admin" {
  backend = "local"
  config = {
    path = "../admin/terraform.tfstate"
  }
}


resource "aws_iam_policy" "worker_policy" {
  name        = "worker_policy"
  description = "Policy for worker nodes to access SSM and EC2 describe instances"
  policy      = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect   = "Allow"
        Action   = [
          "ssm:GetParameter",
          "ssm:PutParameter",
          "ec2:DescribeInstances"
        ]
        Resource = "*"
      }
    ]
  })
}
resource "aws_iam_role" "master_role" {
  name               = "master-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action    = "sts:AssumeRole"
        Effect    = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role" "worker_role" {
  name               = "worker-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action    = "sts:AssumeRole"
        Effect    = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "master_policy_attachment" {
  role       = aws_iam_role.master_role.name
  policy_arn = aws_iam_policy.worker_policy.arn
}

resource "aws_iam_role_policy_attachment" "worker_policy_attachment" {
  role       = aws_iam_role.worker_role.name
  policy_arn = aws_iam_policy.worker_policy.arn
}

resource "aws_iam_instance_profile" "master_instance_profile" {
  name = "master-instance-profile"
  role = aws_iam_role.master_role.name
}

resource "aws_iam_instance_profile" "worker_instance_profile" {
  name = "worker-instance-profile"
  role = aws_iam_role.worker_role.name
}


# Create the Kubernetes master instance
resource "aws_instance" "master" {
  ami           = "ami-052387465d846f3fc"  # Change to an appropriate AMI ID for your region
  instance_type = "t3.medium"
  subnet_id     = data.terraform_remote_state.networking.outputs.public_subnet_id1
  vpc_security_group_ids = [data.terraform_remote_state.networking.outputs.instance_security_group_id]
  key_name      = var.key_name
  associate_public_ip_address = true
  iam_instance_profile        = aws_iam_instance_profile.master_instance_profile.name

  tags = {
    Name  = "K8s-Master"
    Group = "Kubernetes"
  }
}


# Create the worker instances in two subnets for high availability
# Povision worker instances
resource "aws_launch_template" "worker" {
  name_prefix   = "k8s-worker-"
  image_id      = "ami-052387465d846f3fc"  # Change to an appropriate AMI ID for your region
  instance_type = "t3.medium"
  key_name      = var.key_name
  iam_instance_profile {
    name = aws_iam_instance_profile.worker_instance_profile.name
  }

  network_interfaces {
    associate_public_ip_address = true
    security_groups             = [data.terraform_remote_state.networking.outputs.instance_security_group_id]
  }

  tag_specifications {
    resource_type = "instance"
    tags = {
      Name  = "K8s-Worker"
      Group = "Kubernetes"
    }
  }

  user_data = base64encode(<<-EOF
    #!/bin/bash

    # Install necessary packages
    yum update -y
    yum install -y jq dnf-utils

    # Ensure the log directory exists
    mkdir -p /home/ec2-user/devops_setup/terraform/k8s_cluster

    LOG_FILE="/home/ec2-user/devops_setup/terraform/k8s_cluster/terraform_provision_workers.log"

    echo "Starting worker setup script" > $LOG_FILE 2>&1

    # Update all packages
    sudo dnf update -y >> $LOG_FILE 2>&1

    # Disable swap
    sudo swapoff -a >> $LOG_FILE 2>&1

    # Load kernel modules
    echo -e "overlay\nbr_netfilter" | sudo tee /etc/modules-load.d/k8s.conf >> $LOG_FILE 2>&1
    sudo modprobe overlay >> $LOG_FILE 2>&1
    sudo modprobe br_netfilter >> $LOG_FILE 2>&1

    # Set system configurations for Kubernetes
    echo -e "net.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1" | sudo tee /etc/sysctl.d/k8s.conf >> $LOG_FILE 2>&1
    sudo sysctl --system >> $LOG_FILE 2>&1

    # Install containerd
    wget https://github.com/containerd/containerd/releases/download/v1.6.2/containerd-1.6.2-linux-amd64.tar.gz -O /tmp/containerd-1.6.2-linux-amd64.tar.gz >> $LOG_FILE 2>&1
    sudo tar -xvf /tmp/containerd-1.6.2-linux-amd64.tar.gz -C /usr/local >> $LOG_FILE 2>&1
    sudo wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -O /etc/systemd/system/containerd.service >> $LOG_FILE 2>&1
    sudo systemctl enable --now containerd >> $LOG_FILE 2>&1

    # Install runc
    sudo wget https://github.com/opencontainers/runc/releases/download/v1.1.9/runc.amd64 -O /usr/local/sbin/runc >> $LOG_FILE 2>&1
    sudo chmod 755 /usr/local/sbin/runc >> $LOG_FILE 2>&1

    # Install CNI plugins
    sudo mkdir -p /opt/cni/bin >> $LOG_FILE 2>&1
    wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz -O /tmp/cni-plugins-linux-amd64-v1.1.1.tgz >> $LOG_FILE 2>&1
    sudo tar -xvf /tmp/cni-plugins-linux-amd64-v1.1.1.tgz -C /opt/cni/bin >> $LOG_FILE 2>&1

    # Configure containerd
    sudo mkdir -p /etc/containerd >> $LOG_FILE 2>&1
    sudo containerd config default | sudo tee /etc/containerd/config.toml >> $LOG_FILE 2>&1
    sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml >> $LOG_FILE 2>&1
    sudo sed -i 's|k8s.gcr.io/pause:3.6|registry.k8s.io/pause:3.2|' /etc/containerd/config.toml >> $LOG_FILE 2>&1
    sudo systemctl restart containerd >> $LOG_FILE 2>&1

    # Set SELinux to permissive mode
    sudo setenforce 0 >> $LOG_FILE 2>&1
    sudo sed -i 's/^SELINUX=enforcing/SELINUX=permissive/' /etc/selinux/config >> $LOG_FILE 2>&1

    # Add Kubernetes yum repository without exclude
    echo -e "[kubernetes]\nname=Kubernetes\nbaseurl=https://pkgs.k8s.io/core:/stable:/v1.30/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://pkgs.k8s.io/core:/stable:/v1.30/rpm/repodata/repomd.xml.key" | sudo tee /etc/yum.repos.d/kubernetes.repo >> $LOG_FILE 2>&1

    # Install Kubernetes packages
    sudo dnf install -y kubelet kubeadm kubectl >> $LOG_FILE 2>&1

    # Add exclude parameter to Kubernetes yum repository
    echo 'exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni' | sudo tee -a /etc/yum.repos.d/kubernetes.repo >> $LOG_FILE 2>&1

    # Enable and start kubelet
    sudo systemctl enable --now kubelet >> $LOG_FILE 2>&1

    # Install iproute package
    sudo dnf install -y iproute >> $LOG_FILE 2>&1

    # Install iproute-tc package
    sudo dnf install -y iproute-tc >> $LOG_FILE 2>&1

    # Retrieve the join command from SSM Parameter Store
    JOIN_COMMAND=$(aws ssm get-parameter --name "k8s-join-command" --with-decryption --query "Parameter.Value" --output text --region eu-north-1)

    export JOIN_COMMAND

    eval sudo $JOIN_COMMAND

  EOF
  )
}

resource "aws_autoscaling_group" "k8s_asg" {
  depends_on = [null_resource.provision_master]
  
  desired_capacity     = 1
  max_size             = 4
  min_size             = 1
  vpc_zone_identifier  = [
    data.terraform_remote_state.networking.outputs.public_subnet_id1, 
    data.terraform_remote_state.networking.outputs.public_subnet_id2
  ]

  target_group_arns = [data.terraform_remote_state.networking.outputs.lb_target_group_arn]

  tag {
    key                 = "Name"
    value               = "K8s-Worker"
    propagate_at_launch = true
  }

  mixed_instances_policy {
    instances_distribution {
      on_demand_allocation_strategy            = "prioritized"
      on_demand_base_capacity                  = 0
      on_demand_percentage_above_base_capacity = 100
      spot_allocation_strategy                 = "capacity-optimized"
    }

    launch_template {
      launch_template_specification {
        launch_template_id = aws_launch_template.worker.id
        version            = "$Latest"
      }

      override {
        instance_type = "t3.medium"
      }
    }
  }
}

# Local-exec to update the inventory file
resource "null_resource" "update_inventory" {
  depends_on = [aws_instance.master]

  provisioner "local-exec" {
    command = <<-EOT
      WORKER_IPS=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=K8s-Worker" "Name=instance-state-name,Values=running" --query "Reservations[*].Instances[*].PublicIpAddress" --output text)
      /home/ec2-user/devops_setup/terraform/generate_inventory.sh
    EOT
  }

  triggers = {
    master_ip = aws_eip.master_eip.public_ip
  }
}

# Provision master instance
resource "null_resource" "provision_master" {
  depends_on = [aws_instance.master]

  provisioner "local-exec" {
    command = <<EOT
      #!/bin/bash
      set -e

      MASTER_IP=${aws_instance.master.private_ip}
      PRIVATE_KEY_PATH=${var.private_key_path}
      AWS_REGION="eu-north-1"
      PARAMETER_NAME="k8s-join-command"

      # Ensure correct permissions for the private key
      chmod 600 $PRIVATE_KEY_PATH
      eval "$(ssh-agent -s)"
      ssh-add $PRIVATE_KEY_PATH

      # Wait until the instance is available
      while ! nc -z -w5 $MASTER_IP 22; do
        echo "Waiting for instance to be available at IP $MASTER_IP..."
        sleep 10
      done

      # Add the master IP to known hosts to avoid SSH prompt
      ssh-keygen -R $MASTER_IP -f ~/.ssh/known_hosts || true
      ssh-keyscan -H $MASTER_IP >> ~/.ssh/known_hosts

      # Test SSH connection
      echo "Testing SSH connection to master..."
      ssh -o StrictHostKeyChecking=no -i $PRIVATE_KEY_PATH ec2-user@$MASTER_IP "echo 'SSH connection successful'"

      # Run the script on the admin server to execute the Ansible playbook
      echo "Starting the Ansible Playbook for Kubernetes installation on master"
      echo "private key path: $PRIVATE_KEY_PATH"
      bash /home/ec2-user/devops_setup/ansible/playbooks/kubernetes/setup_kubernetes_cluster.sh $MASTER_IP $PRIVATE_KEY_PATH > terraform_provision_master.log 2>&1
      if [ $? -ne 0 ]; then
        echo "Failed to run setup_kubernetes_cluster.sh. Check terraform_provision_master.log for details."
        exit 1
      fi

      # Run the Ansible playbook to configure kubectl authentication
      echo "Starting the Ansible Playbook for kubectl authentication setup"
      bash /home/ec2-user/devops_setup/ansible/playbooks/kubernetes/setup_kubectl_auth.sh $MASTER_IP $PRIVATE_KEY_PATH > kubeconfig_setup.log 2>&1
      if [ $? -ne 0 ]; then
        echo "Failed to run setup_kubectl_auth.sh. Check kubeconfig_setup.log for details."
        exit 1
      fi

      # Retrieve the join command
      JOIN_COMMAND=$(ssh -o StrictHostKeyChecking=no -i $PRIVATE_KEY_PATH ec2-user@$MASTER_IP "sudo kubeadm token create --print-join-command")

      # Store the join command in SSM Parameter Store
      aws ssm put-parameter --name $PARAMETER_NAME --value "$JOIN_COMMAND" --type "String" --overwrite --region $AWS_REGION
    EOT
  }
}




# Allocate Elastic IP for Master Node
resource "aws_eip" "master_eip" {
  instance = aws_instance.master.id
}

# Outputs
output "master_public_ip" {
  value = aws_eip.master_eip.public_ip
}

output "lb_target_group_arn" {
  value = data.terraform_remote_state.networking.outputs.lb_target_group_arn
}

resource "aws_autoscaling_policy" "scale_out_policy" {
  name                   = "scale-out"
  scaling_adjustment     = 1
  adjustment_type        = "ChangeInCapacity"
  cooldown               = 300
  autoscaling_group_name = aws_autoscaling_group.k8s_asg.name
}

resource "aws_autoscaling_policy" "scale_in_policy" {
  name                   = "scale-in"
  scaling_adjustment     = -1
  adjustment_type        = "ChangeInCapacity"
  cooldown               = 300
  autoscaling_group_name = aws_autoscaling_group.k8s_asg.name
}

variables.tf

variable "instance_type" {
  description = "Type of EC2 instance"
  type        = string
  default     = "t3.medium"
}

variable "key_name" {
  description = "Name of the SSH key pair"
  type        = string
  default     = "devOps_training"
}

variable "private_key_path" {
  description = "Path of private key"
  type        = string
  default     = "/home/ec2-user/.ssh/my-key-pair"
}

variable "region" {
  description = "AWS region"
  type        = string
  default     = "eu-north-1"
}

variable "desired_capacity" {
  description = "Desired capacity for ASG"
  type        = number
  default     = 1
}

variable "max_size" {
  description = "Maximum size for ASG"
  type        = number
  default     = 4
}

variable "min_size" {
  description = "Minimum size for ASG"
  type        = number
  default     = 1
}

outputs.tf

output "asg_name" {
  value = aws_autoscaling_group.k8s_asg.name
}

output "scale_out_policy_arn" {
  value = aws_autoscaling_policy.scale_out_policy.arn
}

output "scale_in_policy_arn" {
  value = aws_autoscaling_policy.scale_in_policy.arn
}

terraform.tfvars

instance_type = "t3.medium"
key_name = "devOps_training"
private_key_path = "/home/ec2-user/.ssh/my-key-pair"
region = "eu-north-1"
desired_capacity = 1
max_size = 4
min_size = 1

----------------------

Notifications

main.tf

provider "aws" {
  region = var.region
}

data "aws_caller_identity" "current" {}

data "terraform_remote_state" "k8s_cluster" {
  backend = "local"
  config = {
    path = "../k8s_cluster/terraform.tfstate"
  }
}

resource "aws_iam_role" "autoscaling_role" {
  name = "autoscaling_role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Principal = {
          Service = "autoscaling.amazonaws.com"
        },
        Action = "sts:AssumeRole"
      }
    ]
  })
}

resource "aws_iam_role_policy" "autoscaling_policy" {
  name   = "autoscaling_policy"
  role   = aws_iam_role.autoscaling_role.id
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect   = "Allow",
        Action   = [
          "autoscaling:DescribeAutoScalingGroups",
          "autoscaling:UpdateAutoScalingGroup",
          "cloudwatch:PutMetricAlarm",
          "cloudwatch:DescribeAlarms",
          "ec2:DescribeInstances"
        ],
        Resource = "*"
      }
    ]
  })
}

resource "aws_cloudwatch_metric_alarm" "high_cpu" {
  alarm_name          = "high-cpu-alarm"
  comparison_operator = "GreaterThanOrEqualToThreshold"
  evaluation_periods  = 21
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = 120
  statistic           = "Average"
  threshold           = 50
  alarm_description   = "This metric monitors the average CPU usage for the ASG."
  dimensions = {
    AutoScalingGroupName = data.terraform_remote_state.k8s_cluster.outputs.asg_name
  }
  alarm_actions = [data.terraform_remote_state.k8s_cluster.outputs.scale_out_policy_arn]
}

resource "aws_cloudwatch_metric_alarm" "low_cpu" {
  alarm_name          = "low-cpu-alarm"
  comparison_operator = "LessThanOrEqualToThreshold"
  evaluation_periods  = 2
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = 120
  statistic           = "Average"
  threshold           = 20
  alarm_description   = "This metric monitors the average CPU usage for the ASG."
  dimensions = {
    AutoScalingGroupName = data.terraform_remote_state.k8s_cluster.outputs.asg_name
  }
  alarm_actions = [data.terraform_remote_state.k8s_cluster.outputs.scale_in_policy_arn]
}

variaboes.tf

variable "region" {
  description = "AWS region"
  type        = string
  default     = "eu-north-1"
}
